# Importing the Dependencies
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib
import nltk
import pickle

# Download required NLTK resources
nltk.download('stopwords')

# Data Pre-processing
# Loading the dataset to a pandas DataFrame
news_dataset = pd.read_csv(r'C:\Users\Jigyasa\Desktop\Project\dataset\train.csv', encoding='unicode_escape')

# Counting the number of missing values in the dataset
print(news_dataset.isnull().sum())

# Replacing the null values with empty string
news_dataset = news_dataset.fillna('')

# Merging the author name and news title into a new column 'content'
news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']

# Separating the data & label
X = news_dataset['content'].values
Y = news_dataset['label'].values

# Stemming
port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)
    stemmed_content = stemmed_content.lower().split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if word not in stopwords.words('english')]
    return ' '.join(stemmed_content)

# Applying stemming
news_dataset['content'] = news_dataset['content'].apply(stemming)

# Reassigning X and Y after processing
X = news_dataset['content'].values
Y = news_dataset['label'].values

# Converting the textual data to numerical data
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(X)

# Splitting the dataset into training & test data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

# Training the Model: Logistic Regression
model = LogisticRegression()
model.fit(X_train, Y_train)

# Evaluation: Accuracy score
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)
print('Accuracy score of the training data : ', training_data_accuracy)

X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)
print('Accuracy score of the test data : ', test_data_accuracy)

# Making a Predictive System
X_new = X_test[3]
prediction = model.predict(X_new)

if prediction[0] == 0:
    print('The news is Real')
else:
    print('The news is Fake')
print(Y_test[3])

# Saving the model and vectorizer
model_filename = 'trained_model.sav'
joblib.dump(model, model_filename)  # Saving the trained model
vectorizer_filename = 'tfidf_vectorizer.pkl'
joblib.dump(vectorizer, vectorizer_filename)  # Saving the vectorizer

print("Model and vectorizer saved successfully.")
